{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlproject.components import load_data, DataExplore, DataProcess, DataVisualize, save_to_csv\n",
    "from mlproject.model import logistic_regression, decision_tree, \\\n",
    "knn, random_forest, xgboost , split_data, auto_encoder\n",
    "from typing import Union, List\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(file_path=\"Dataset/creditcard_2023.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = data.select_dtypes(include=\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated(keep=\"last\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop duplicate rows if you find any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using another Faster method to find Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().values.any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Fill Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fill row with Mean or Median\n",
    "def fill_null(data: pd.DataFrame, method: Union[List[str]] = \"mean\" ): \n",
    "    if method == \"zero\":\n",
    "        print(\"using zero\")\n",
    "        data.fillna(value=0, inplace=True)\n",
    "    elif method == \"mean\":\n",
    "        print(\"using mean\")\n",
    "        for col in data.columns:\n",
    "            data[col] = data[col].fillna(value=data[col].mean())\n",
    "    else:\n",
    "        for col in data.columns:\n",
    "            data[col] = data[col].fillna(value=data[col].median())\n",
    "    return data\n",
    "\n",
    "#fill_null(data=data, method=\"zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing id column which is just a number\n",
    "data.drop(\"id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Outliers & Visualize it\n",
    "\n",
    "Method to find Outliers:\n",
    "* Z- Score\n",
    "* Quantile filter\n",
    "* IQR - Distance from Median - Below example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outliers(data: pd.DataFrame):\n",
    "\n",
    "    outliers = []\n",
    "\n",
    "    for col in data.select_dtypes(include=\"number\").drop(columns=[\"Amount\", \"Class\"]).columns:\n",
    "        Q1 = data[col].quantile(0.25)\n",
    "        Q3 = data[col].quantile(0.75)\n",
    "\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "        #print(Q1,Q3,IQR, lower_bound, upper_bound)\n",
    "\n",
    "        # Z-Score implementation\n",
    "        threshold = 3\n",
    "        #Step1: Calculated Mean\n",
    "        mean = data[col].mean()\n",
    "        # Step2: Squarred differences\n",
    "        squared_diff = (data[col] - mean)**2\n",
    "        # Step3: Divide Squarred diff with lenght of column\n",
    "        variance = squared_diff.sum() / len(data[col])\n",
    "        #Step4: Standard Deviation\n",
    "        std = variance ** 0.5\n",
    "        z_score = (data[col] - mean)/std\n",
    "        outliers_zscore =  data[col] [z_score.abs() > threshold]\n",
    "        #########################################################\n",
    "\n",
    "        outlier_mask = (data[col] < lower_bound) | (data[col] > upper_bound)\n",
    "\n",
    "        outliers_data = data[col][outlier_mask]\n",
    "\n",
    "        data.loc[outlier_mask, col] = np.nan\n",
    "\n",
    "        num_outliers = len(outliers_data)\n",
    "        percent_outliers = (num_outliers / len(data[col])) * 100\n",
    "        #if percent_outliers > 1.0:\n",
    "        outliers.append([data[col].name, data[col].shape[0], num_outliers,\"num:\",round(percent_outliers, 3), \"%\", len(outliers_zscore),round(lower_bound, 3),round(upper_bound,3)])\n",
    "    return outliers, data\n",
    "\n",
    "_ , data = check_outliers(data=data)\n",
    "\n",
    "#plt.figure(figsize=(10,8)) # widthx Height\n",
    "#sns.boxplot(data=data.drop(columns= [\"Amount\", \"Class\"])) # ignore Nan values \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping All Nan contained rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8)) # widthx Height\n",
    "sns.boxplot(data=data.drop(columns= [\"Amount\", \"Class\"])) # ignore Nan values \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling 0 -> 1\n",
    "* Note: Before scaling, let's check data distribution using Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only numerical columns\n",
    "numerical_data = data.drop(columns= [\"Amount\", \"Class\"])\n",
    "\n",
    "# Number of columns in the plot grid\n",
    "num_cols = 3\n",
    "num_rows = len(numerical_data.columns) // num_cols + (len(numerical_data.columns) % num_cols != 0)\n",
    "\n",
    "# Set up the figure with subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# Flatten axes array to easily iterate\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop over columns to plot histograms\n",
    "for i, col in enumerate(numerical_data.columns):\n",
    "    sns.histplot(data[col], bins=10, kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Histogram of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Hide any empty subplots (if number of columns is not a perfect multiple of num_cols)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Standard Scalar Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "for col in data.drop(columns=[\"Amount\", \"Class\"]).columns:\n",
    "    if data[col].isnull().any():\n",
    "        data[col] = data[col].fillna(data[col].mean())\n",
    "    scaled_data = standard_scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    data[col] = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Amount'] = np.log1p(data['Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a 2x2 grid of subplots (adjust as needed)\n",
    "plt.figure(figsize=(20, 16))\n",
    "for i, col in enumerate(data.drop(columns=[\"Class\"]).columns):\n",
    "    plt.subplot(5, 6, i+1)  # Adjust this layout based on number of columns\n",
    "    sns.histplot(data[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking class values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data['Class'].value_counts()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check corr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(data=data.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns = [\"Class\"])\n",
    "y = data[\"Class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call models\n",
    "results = {\n",
    "    \"Logistic Regression\": logistic_regression(X_train, y_train, X_test, y_test),\n",
    "    \"Decision Tree\": decision_tree(X_train, y_train, X_test, y_test),\n",
    "    \"KNN\": knn(X_train, y_train, X_test, y_test),\n",
    "    \"Random Forest\": random_forest(X_train, y_train, X_test, y_test),\n",
    "    \"XGBoost\": xgboost(X_train, y_train, X_test, y_test),\n",
    "}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "Accuracy = accuracy_score(y_test, y_pred)\n",
    "Precision =  precision_score(y_test, y_pred)\n",
    "Recall = recall_score(y_test, y_pred)\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "\n",
    "print(Accuracy, Precision, Recall, F1Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=[\"Class 0\", \"Class 1\"], \n",
    "            yticklabels=[\"Class 0\", \"Class 1\"])\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted\n",
    "    0        1\n",
    "* True 0  [TN: 100, FP: 10]\n",
    "* 1  [FN: 20, TP: 90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest and XGBoost outperform the other models in all metrics, particularly in precision and recall. They are both strong candidates if you are looking for the most reliable models.\n",
    "* KNN and Decision Tree also perform very well, with KNN having perfect recall, but slightly lower precision than Random Forest and XGBoost.\n",
    "* Logistic Regression, while still very good, has the lowest performance of all models, particularly in recall.\n",
    "\n",
    "* Based on these results, Random Forest and XGBoost are the top-performing models in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "* All are Numerical Columns with shape (568630, 31)\n",
    "* All Numerical columns, No Categorical\n",
    "* All are int64 and Float64, No Object type Numerical values\n",
    "* No duplicate rows & Null values\n",
    "* id column just a numbers, not useful for Model training, now 30 columns only\n",
    "* Heat map is not providing with much info and I didn't find much correlation with among each columns\n",
    "* Calculated Number of Outliers using 3 Sigma Method and Visualized as well using Seaboarn boxplot\n",
    "* Replaced outliers with Nan values and then removed entire rows after that.\n",
    "* Data is normally distributed based on each Histogram\n",
    "* Apply feature scaling using standard scalar due to Normal distrubution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
